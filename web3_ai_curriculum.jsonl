[
  {
    "question": "1주차: Web3의 핵심 철학은 무엇이며, '읽기-쓰기-소유(Read-Write-Own)'라는 개념은 어떻게 설명되나요?",
    "answer": "Web3의 핵심 철학은 인터넷의 권력을 중앙화된 거대 플랫폼으로부터 사용자 개개인에게 되돌려주는 **'사용자 주권'**에 있습니다. 이는 '읽기-쓰기-소유(Read-Write-Own)'라는 개념으로 구체화됩니다.\n\n- **Web1 (Read-Only):** 1990년대의 초기 인터넷으로, 대부분의 사용자는 정적인 웹페이지의 정보를 '읽기'만 할 수 있었습니다.\n- **Web2 (Read-Write):** 2000년대 이후의 소셜 미디어 시대로, 사용자는 플랫폼(페이스북, 유튜브 등)에 콘텐츠를 '쓰고' 공유하며 적극적으로 참여하게 되었습니다. 하지만 이 데이터의 소유권과 통제권은 플랫폼 기업에 귀속되었습니다.\n- **Web3 (Read-Write-Own):** 현재 진화하고 있는 차세대 인터넷으로, 사용자는 블록체인 기술을 통해 자신이 생성한 데이터, 콘텐츠, 디지털 자산을 직접 **'소유(Own)'**하고 통제할 수 있습니다. 이는 중앙 서버가 아닌, 탈중앙화된 네트워크 위에 데이터가 기록되기 때문에 가능합니다.\n\n**Reference:**\n- 'What Is Web3?' (https://www.forbes.com/crypto-blockchain/2022/07/26/what-is-web3/)"
  },
  {
    "question": "1주차: 중앙화된 AI가 가진 '데이터 사일로(Data Silo)' 문제가 무엇이며, 탈중앙화 AI는 이를 어떻게 해결할 수 있나요?",
    "answer": "'데이터 사일로'는 데이터가 특정 조직이나 플랫폼 내부에 고립되어 외부와 공유되거나 통합되지 못하는 현상을 말합니다. 중앙화된 AI 모델은 자신이 속한 기업의 데이터로만 학습되므로, 다양한 데이터를 활용하지 못해 성능 향상에 한계가 있고 편향된 결과를 낳을 수 있습니다.\n\n탈중앙화 AI는 이 문제를 다음과 같이 해결합니다.\n\n1.  **연합 학습 (Federated Learning):** 각 사용자의 데이터를 중앙 서버로 보내지 않고, 사용자의 디바이스(엣지)에서 로컬로 모델을 학습시킵니다. 이후 학습된 모델의 일부(가중치 등)만을 취합하여 전체 모델을 업데이트합니다. 이를 통해 개인정보를 보호하면서도 다양한 데이터를 학습에 활용할 수 있습니다.\n2.  **데이터 마켓플레이스 (Data Marketplace):** 사용자가 자신의 데이터에 대한 접근 권한을 탈중앙화된 시장에서 직접 판매하거나 제공하고, 이에 대한 보상을 받을 수 있습니다. 이는 데이터가 필요한 AI 개발자와 데이터를 가진 사용자를 안전하게 연결하여 사일로를 허물어뜨립니다.\n\n**Reference:**\n- 'Federated Learning: Collaborative Machine Learning without Centralized Training Data' (https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)"
  },
  {
    "question": "2주차: MCP(모델-컨텍스트-프로토콜) 아키텍처의 구성요소인 'Host', 'Client', 'Server'의 역할은 각각 무엇인가요?",
    "answer": "MCP 아키텍처는 세 가지 주요 구성요소로 이루어져 있으며, 각자의 역할이 명확히 구분됩니다.\n\n- **Server (서버):** 외부 컨텍스트를 제공하는 주체입니다. 즉, AI 모델이 사용하고자 하는 도구(Tool)나 데이터(Resource)를 실제로 가지고 있으며, 이를 MCP 표준에 맞춰 외부에 노출합니다. 예를 들어, 사내 데이터베이스 API를 래핑하여 MCP 서버로 만들 수 있습니다.\n\n- **Client (클라이언트):** AI 모델(LLM)을 포함하는 애플리케이션으로, 외부 컨텍스트가 필요한 주체입니다. 클라이언트는 서버에 특정 도구의 사용이나 데이터 조회를 요청(Request)하고, 그 결과를 받아 자신의 작업을 수행합니다.\n\n- **Host (호스트):** 클라이언트와 서버 사이의 통신을 중개하고 관리하는 논리적인 컴포넌트입니다. 호스트는 클라이언트의 요청을 받아 해당 요청을 처리할 수 있는 적절한 서버로 전달하고, 서버의 응답을 다시 클라이언트에게 돌려줍니다. 또한, 인증, 권한 부여, 로깅 등 공통적인 기능을 처리하여 클라이언트와 서버가 핵심 비즈니스 로직에만 집중할 수 있도록 돕습니다. AWS 같은 클라우드 제공업체가 호스트 역할을 수행할 수 있습니다.\n\n**Reference:**\n- 'Unlocking the power of Model-Context-Protocol (MCP) on AWS'의 아키텍처 다이어그램 부분 (https://aws.amazon.com/blogs/machine-learning/unlocking-the-power-of-model-context-protocol-mcp-on-aws/)"
  },
  {
    "question": "2주차: MCP에서 'Tool'과 'Resource'의 차이점은 무엇인가요? 각각의 예시를 들어 설명해주세요.",
    "answer": "MCP에서 'Tool'과 'Resource'는 모두 AI 에이전트가 사용하는 외부 기능이지만, 그 성격에 차이가 있습니다.\n\n- **Tool (도구):** **능동적인 작업(Action)을 수행**하는 기능입니다. 입력값을 받아 어떤 프로세스를 실행하고 결과를 반환합니다. 즉, 동사(Verb)에 가깝습니다.\n    - **예시:**\n        - `send_email(recipient, subject, body)`: 이메일을 발송하는 도구.\n        - `calculate_exchange_rate(from_currency, to_currency)`: 환율을 계산하는 도구.\n        - `book_flight(destination, date)`: 항공권을 예약하는 도구.\n\n- **Resource (자원):** **정적인 데이터나 정보에 대한 접근**을 제공합니다. CRUD(Create, Read, Update, Delete)와 같은 데이터 관리 작업을 수행하며, 명사(Noun)에 가깝습니다.\n    - **예시:**\n        - `customer_database`: 고객 정보 데이터베이스에 접근하여 특정 고객 정보를 조회하거나 수정.\n        - `product_catalog`: 제품 카탈로그에 접근하여 특정 제품의 재고나 가격을 확인.\n        - `calendar_events`: 사용자의 캘린더에 접근하여 특정 날짜의 일정을 조회하거나 새로운 일정을 추가.\n\nAI 에이전트는 '고객 데이터베이스(Resource)에서 VIP 고객 목록을 조회한 후, 그들에게 할인 쿠폰 이메일을 보내라(Tool)'와 같이 자원과 도구를 조합하여 복잡한 작업을 수행할 수 있습니다.\n\n**Reference:**\n- `jlowin/fastmcp` GitHub Repository의 예제 코드 (https://github.com/jlowin/fastmcp)"
  },
  {
    "question": "3주차: A2A 프로토콜의 '에이전트 카드(Agent Card)'에는 구체적으로 어떤 정보가 포함되어야 하나요?",
    "answer": "'에이전트 카드'는 A2A 네트워크상에서 해당 에이전트의 '명함'과 같은 역할을 합니다. 다른 에이전트가 이 카드를 보고 해당 에이전트의 정체, 능력, 통신 방법을 파악할 수 있어야 합니다. 카드에는 일반적으로 다음과 같은 정보가 포함됩니다.\n\n- **`agent_id`**: 네트워크 내에서 에이전트를 고유하게 식별하는 ID.\n- **`name`**: 사람이 읽을 수 있는 에이전트의 이름 (예: '날씨 정보 전문 에이전트').\n- **`description`**: 에이전트의 역할과 목적을 설명하는 상세한 문장.\n- **`capabilities`**: 에이전트가 수행할 수 있는 작업(Task)들의 목록. 각 작업은 다음을 포함할 수 있습니다.\n    - `task_name`: 작업의 이름 (예: `get_current_weather`).\n    - `description`: 작업에 대한 설명.\n    - `input_schema`: 작업을 수행하는 데 필요한 입력 파라미터의 JSON 스키마.\n    - `output_schema`: 작업 완료 시 반환되는 출력값의 JSON 스키마.\n- **`endpoint`**: 에이전트와 통신할 수 있는 URL 주소.\n- **`authentication`**: 해당 에이전트와 통신하는 데 필요한 인증 방법 (예: API 키, OAuth 2.0).\n- **`protocol_version`**: 지원하는 A2A 프로토콜의 버전.\n\n이러한 표준화된 정보 덕분에, 어떤 에이전트든 네트워크에 처음 참여하는 다른 에이전트의 기능을 동적으로 파악하고 즉시 협업을 시작할 수 있습니다.\n\n**Reference:**\n- 'Orchestrating Heterogeneous and Distributed Multi-Agent Systems Using Agent-to-Agent (A2A) Protocol' (https://fractal.ai/blog/orchestrating-heterogeneous-and-distributed-multi-agent-systems-using-agent-to-agent-a2a-protocol/)"
  },
  {
    "question": "3주차: A2A 프로토콜에서 통신을 위해 JSON-RPC를 사용하는 이유는 무엇인가요? 일반적인 REST API 방식과 비교했을 때의 장점은 무엇인가요?",
    "answer": "A2A 프로토콜이 통신 방식으로 JSON-RPC를 채택한 데에는 몇 가지 중요한 이유가 있으며, 이는 REST API 방식과 비교했을 때 장점이 될 수 있습니다.\n\n**JSON-RPC (JSON Remote Procedure Call)**는 원격에 있는 서버의 함수(Procedure)를 로컬에 있는 것처럼 호출하는 간단한 프로토콜입니다. A2A에서 JSON-RPC를 사용하는 이유는 다음과 같습니다.\n\n1.  **경량성 및 단순함:** REST는 HTTP 메서드(GET, POST, PUT, DELETE)와 URL 구조를 통해 리소스를 표현하는 등 다소 복잡한 규칙을 갖지만, JSON-RPC는 `method`(호출할 함수 이름), `params`(전달할 인자), `id`(요청 식별자)라는 단순한 구조의 JSON 객체를 POST 요청 하나로 보내면 됩니다. 이는 에이전트 간의 통신 로직을 매우 간단하게 만듭니다.\n\n2.  **명확한 행위 정의:** REST는 리소스(명사) 중심적인 반면, JSON-RPC는 프로시저(동사) 중심적입니다. 에이전트 간의 상호작용은 '데이터 조회'보다는 '작업 요청'이나 '기능 실행'과 같은 행위(Action) 중심인 경우가 많으므로, 함수를 직접 호출하는 방식의 JSON-RPC가 더 직관적입니다.\n\n3.  **양방향 통신 용이성:** HTTP(S) 외에도 WebSocket 등 다양한 전송 계층 위에서 동작할 수 있어, 서버가 클라이언트의 함수를 호출하는 등의 양방향 통신 시나리오를 구현하기에 더 유연합니다.\n\n반면, REST API는 웹 캐싱, 표준화된 상태 코드 등 HTTP의 기능을 최대한 활용하는 장점이 있어 웹 기반의 리소스 중심 서비스에 더 적합할 수 있습니다. A2A는 에이전트 간의 **기능 호출**에 초점을 맞추었기 때문에 JSON-RPC가 더 적합한 선택이었다고 볼 수 있습니다.\n\n**Reference:**\n- 'JSON-RPC 2.0 Specification' (https://www.jsonrpc.org/specification)\n- 'Understanding A2A: The Protocol for Agent Collaboration' (https://googlecloudcommunity.com/gc/Community-Blogs/Understanding-A2A-The-Protocol-for-Agent-Collaboration/ba-p/906323)"
  },
  {
    "question": "4주차: AI 에이전트의 '단기 기억'은 구체적으로 무엇을 의미하며, '컨텍스트 창(Context Window)'의 한계는 무엇인가요?",
    "answer": "AI 에이전트의 '단기 기억'은 현재 진행 중인 대화나 작업의 맥락을 파악하는 능력을 말합니다. 이는 주로 LLM 모델의 **'컨텍스트 창(Context Window)'**에 의해 구현됩니다. 컨텍스트 창은 모델이 한 번에 처리할 수 있는 텍스트의 최대 길이(토큰 수)를 의미합니다.\n\n예를 들어, 컨텍스트 창이 4,000 토큰인 모델은 현재 대화의 최근 4,000 토큰만큼의 내용만 '기억'할 수 있습니다. 이 창 안에 있는 정보는 모델이 직접 접근하여 답변을 생성하는 데 활용합니다.\n\n**컨텍스트 창의 한계:**\n\n1.  **기억 상실:** 대화가 길어져 컨텍스트 창의 크기를 넘어서면, 가장 오래된 정보부터 순차적으로 '잊어버리게' 됩니다. 1시간 전에 나눴던 중요한 대화 내용을 기억하지 못하는 문제가 발생할 수 있습니다.\n2.  **비용 및 속도 문제:** 컨텍스트 창이 클수록 더 많은 계산량이 필요하므로, 응답 속도가 느려지고 API 사용 비용이 급격히 증가합니다.\n3.  **'Lost in the Middle' 현상:** 연구에 따르면, LLM은 컨텍스트 창의 처음과 끝에 있는 정보는 잘 기억하지만, 중간에 있는 정보는 놓치는 경향이 있습니다. 중요한 정보가 중간에 위치하면 제대로 활용하지 못할 수 있습니다.\n\n이러한 한계 때문에 단기 기억만으로는 부족하며, 중요한 정보를 영구적으로 저장하고 필요할 때 다시 꺼내 쓰는 '장기 기억' 메커니즘(예: RAG)이 반드시 필요합니다.\n\n**Reference:**\n- 'Attention Is All You Need' (The original Transformer paper) (https://arxiv.org/abs/1706.03762)\n- 'Lost in the Middle: How Language Models Use Long Contexts' (https://arxiv.org/abs/2307.03172)"
  },
  {
    "question": "4주차: RAG(검색 증강 생성)의 작동 과정을 'Ingestion', 'Retrieval', 'Augmentation & Generation'의 3단계로 나누어 설명해주세요.",
    "answer": "RAG는 LLM이 외부 지식 소스를 활용하여 더 정확하고 풍부한 답변을 생성하도록 하는 기술이며, 크게 3단계로 작동합니다.\n\n1.  **Ingestion (데이터 수집 및 처리):**\n    - **Chunking:** 원본 문서(PDF, 웹페이지, 텍스트 파일 등)를 의미적으로 관련된 작은 덩어리(Chunk)로 분할합니다.\n    - **Embedding:** 각 청크를 임베딩 모델을 사용해 고차원의 숫자 벡터로 변환합니다. 이 벡터는 해당 청크의 의미적 내용을 압축하여 담고 있습니다.\n    - **Indexing:** 변환된 벡터와 원본 텍스트 청크를 **벡터 데이터베이스(Vector DB)**에 저장하고 인덱싱합니다. 이 과정은 '지식 베이스'를 구축하는 단계로, 보통 오프라인에서 미리 수행됩니다.\n\n2.  **Retrieval (관련 정보 검색):**\n    - **Query Embedding:** 사용자의 질문(Query)이 들어오면, Ingestion 단계에서 사용한 것과 동일한 임베딩 모델로 질문을 벡터로 변환합니다.\n    - **Vector Search:** 벡터 데이터베이스에서 사용자의 질문 벡터와 의미적으로 가장 유사한(코사인 유사도 등이 높은) 상위 K개의 텍스트 청크 벡터를 검색합니다.\n    - **Fetch Chunks:** 검색된 벡터에 해당하는 원본 텍스트 청크들을 가져옵니다.\n\n3.  **Augmentation & Generation (정보 증강 및 답변 생성):**\n    - **Prompt Augmentation:** 검색된 텍스트 청크들(컨텍스트)을 사용자의 원본 질문과 함께 프롬프트 템플릿에 삽입하여, LLM에게 전달할 최종 프롬프트를 구성합니다.\n    - **Generation:** LLM은 이 '증강된 프롬프트'를 입력받아, 주어진 컨텍스트 정보를 바탕으로 최종 답변을 생성합니다. 이를 통해 LLM은 자신의 내부 지식에만 의존하지 않고, 외부의 최신 정보를 반영하여 환각을 줄이고 사실에 기반한 답변을 할 수 있게 됩니다.\n\n**Reference:**\n- 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks' (The original RAG paper) (https://arxiv.org/abs/2005.11401)\n- 'What is retrieval-augmented generation?' (https://research.ibm.com/blog/what-is-retrieval-augmented-generation)"
  },
  {
    "question": "5주차: '합의(Consensus)' 메커니즘이 환각을 줄이는 데 효과적인 이유는 무엇이며, 이 방식의 잠재적인 단점이나 한계는 무엇인가요?",
    "answer": "'합의(Consensus)' 메커니즘이 환각을 줄이는 데 효과적인 주된 이유는 **'집단 지성의 원리'**를 적용하기 때문입니다. 한 명의 전문가가 실수할 수 있지만, 여러 전문가가 독립적으로 검토하고 토론하면 오류를 발견하고 수정할 확률이 크게 높아집니다. AI 에이전트에게도 이 원리가 동일하게 적용됩니다.\n\n**효과적인 이유:**\n- **다각도 검증:** 서로 다른 관점이나 데이터를 가진 여러 에이전트가 동일한 문제에 대한 답을 생성함으로써, 단일 에이전트의 편향이나 잘못된 추론으로 인한 환각이 다른 에이전트에 의해 지적되고 걸러질 수 있습니다.\n- **사실 확인 트리거:** 에이전트 간의 답변이 불일치할 경우, 이를 '의심스러운 상황'으로 간주하고 외부 데이터 소스를 통해 사실 확인(Fact-checking)을 수행하도록 자동화할 수 있습니다.\n- **신뢰도 향상:** 여러 에이전트가 만장일치로 동의한 답변은 단일 에이전트의 답변보다 훨씬 높은 신뢰도를 가집니다.\n\n**잠재적 단점 및 한계:**\n- **비용 증가:** 여러 에이전트를 동시에 실행해야 하므로, API 호출 비용과 계산 리소스가 몇 배로 증가합니다.\n- **응답 속도 저하:** 에이전트 간 토론, 투표, 사실 확인 등 여러 단계를 거치므로 최종 답변이 나오기까지 시간이 더 오래 걸립니다.\n- **집단 편향 (Groupthink):** 초기 발언권이 센 에이전트의 의견으로 다른 에이전트들이 동조하거나, 모든 에이전트가 동일하게 편향된 데이터로 학습했을 경우, 만장일치로 틀린 답(환각)을 내놓을 수도 있습니다. 합의가 항상 진실을 보장하지는 않습니다.\n- **합의 실패:** 에이전트 간의 의견 차이가 너무 커서 최종 합의에 도달하지 못하고 교착 상태에 빠질 수 있습니다.\n\n**Reference:**\n- 'A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning' (https://arxiv.org/abs/2405.02058)"
  }
]
